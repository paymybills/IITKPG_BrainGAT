{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f189a9e9",
   "metadata": {},
   "source": [
    "Objective: Use the ABIDE dataset to explore a GAT for prediction of autism (imaging features only, no phenotypes). Leveraging CUDA on RTX 2050."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a36820",
   "metadata": {},
   "source": [
    "We're looking at a paper that proposed this architecture \n",
    "\n",
    "To fully utilize the phenotype information, this paper proposes a heterogeneous graph convolutional attention network (HCAN) model to classify ASD. By combining an attention mechanism and a heterogeneous graph convolutional network, important aggregated features can be extracted in the HCAN. The model consists of a multilayer HCAN feature extractor and a multilayer perceptron (MLP) classifier. First, a heterogeneous population graph was constructed based on the fMRI and phenotypic data. Then, a multilayer HCAN is used to mine graph-based features from the heterogeneous graph. Finally, the extracted features are fed into an MLP for the final classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d64ba1",
   "metadata": {},
   "source": [
    "But the thing is, we're looking at phenotypic data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Attention Network for ASD Classification (NO PHENOTYPIC DATA)\n",
    "# Inspired by HCAN but using ONLY imaging-derived features\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check for CUDA/GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"   Note: Running on CPU - training will be slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2491a",
   "metadata": {},
   "source": [
    "## Graph Attention Network Architecture (Pure Imaging)\n",
    "\n",
    "Instead of HCAN's heterogeneous graph with phenotypic nodes, we use:\n",
    "\n",
    "### Our Approach: Homogeneous Graph with Attention\n",
    "\n",
    "1. **Graph Construction**: Each subject = one graph\n",
    "   - **Nodes**: ROIs (brain regions) \n",
    "   - **Edges**: Functional connectivity (correlation-based)\n",
    "   - **Node Features**: Correlation vectors (purely fMRI-derived)\n",
    "\n",
    "2. **Multi-Head Graph Attention Layers**:\n",
    "   - Replace heterogeneous convolution with **GAT (Graph Attention)**\n",
    "   - Learns which ROI connections are important\n",
    "   - Multi-head attention for diverse feature extraction\n",
    "\n",
    "3. **Hierarchical Feature Extraction**:\n",
    "   - Layer 1: Local connectivity patterns\n",
    "   - Layer 2: Higher-order brain network structure\n",
    "   - Layer 3: Global integration features\n",
    "\n",
    "4. **Readout + MLP Classifier**:\n",
    "   - Aggregate graph-level features\n",
    "   - MLP for final ASD vs Control classification\n",
    "\n",
    "**Key Difference**: HCAN uses phenotype nodes in a population graph. We use **attention within each brain graph** instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading Functions (Fixed & Robust)\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def load_timeseries_1d(path: str) -> np.ndarray:\n",
    "    \"\"\"Load .1D file as T\u00d7N array\"\"\"\n",
    "    arr = np.loadtxt(path)\n",
    "    return arr\n",
    "\n",
    "def corr_matrix(timeseries: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute Pearson correlation matrix\"\"\"\n",
    "    C = np.corrcoef(timeseries, rowvar=False)\n",
    "    C = np.nan_to_num(C, nan=0.0)\n",
    "    C = np.clip(C, -1.0, 1.0)\n",
    "    return C\n",
    "\n",
    "def build_sparse_edges_from_corr(C: np.ndarray, k: int = 20):\n",
    "    \"\"\"Create sparse edge_index via top-k correlations\"\"\"\n",
    "    N = C.shape[0]\n",
    "    np.fill_diagonal(C, 0.0)\n",
    "    \n",
    "    idx_src, idx_dst, weights = [], [], []\n",
    "    absC = np.abs(C)\n",
    "    k_eff = min(k, max(1, N - 1))\n",
    "    \n",
    "    for i in range(N):\n",
    "        nbrs = np.argpartition(absC[i], -k_eff)[-k_eff:]\n",
    "        for j in nbrs:\n",
    "            if i != j:\n",
    "                idx_src.append(i)\n",
    "                idx_dst.append(j)\n",
    "                weights.append(C[i, j])\n",
    "    \n",
    "    # Make undirected\n",
    "    idx_all = np.concatenate([np.vstack([idx_src, idx_dst]), \n",
    "                              np.vstack([idx_dst, idx_src])], axis=1)\n",
    "    w_all = np.array(weights + weights, dtype=np.float32)\n",
    "    \n",
    "    # Deduplicate\n",
    "    pairs = set()\n",
    "    uniq_src, uniq_dst, uniq_w = [], [], []\n",
    "    for (s, d), w in zip(idx_all.T, w_all):\n",
    "        key = (int(s), int(d))\n",
    "        if key not in pairs and s != d:\n",
    "            pairs.add(key)\n",
    "            uniq_src.append(s)\n",
    "            uniq_dst.append(d)\n",
    "            uniq_w.append(w)\n",
    "    \n",
    "    edge_index = torch.tensor([uniq_src, uniq_dst], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(uniq_w, dtype=torch.float)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "def graph_from_timeseries(timeseries: np.ndarray, topk: int = 20):\n",
    "    \"\"\"Build PyG Data from fMRI timeseries\"\"\"\n",
    "    C = corr_matrix(timeseries)\n",
    "    N = C.shape[0]\n",
    "    \n",
    "    # Node features: correlation vectors (purely imaging-derived)\n",
    "    x = torch.tensor(C, dtype=torch.float)\n",
    "    \n",
    "    # Sparse edges\n",
    "    edge_index, edge_attr = build_sparse_edges_from_corr(C.copy(), k=topk)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "print(\"Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f69c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Graph Attention Network for ASD Classification\n",
    "# NO PHENOTYPIC DATA - Pure imaging features only!\n",
    "\n",
    "class MultiHeadGATLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head Graph Attention Layer\n",
    "    Replaces HCAN's heterogeneous conv with attention mechanism\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, heads=8, dropout=0.3, concat=True):\n",
    "        super().__init__()\n",
    "        self.gat = GATConv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            concat=concat,\n",
    "            edge_dim=1  # Use edge weights (correlation values)\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(out_channels * heads if concat else out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        # Multi-head attention aggregation\n",
    "        x = self.gat(x, edge_index, edge_attr=edge_attr)\n",
    "        x = self.bn(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BrainGAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Network for Brain Connectivity Analysis\n",
    "    \n",
    "    Architecture (HCAN-inspired, NO phenotypic data):\n",
    "    - Multi-layer GAT feature extractor (attention-based aggregation)\n",
    "    - Readout: mean + max pooling\n",
    "    - MLP classifier\n",
    "    \n",
    "    Key differences from HCAN:\n",
    "    - Homogeneous graph (only ROI nodes, no phenotype nodes)\n",
    "    - Pure imaging features (correlation-based)\n",
    "    - Multi-head attention for learning important connections\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 hidden_channels=256,\n",
    "                 num_layers=3,\n",
    "                 heads=8,\n",
    "                 dropout=0.3,\n",
    "                 num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Multi-layer GAT feature extractor\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        # First layer: in_channels -> hidden_channels\n",
    "        self.convs.append(\n",
    "            MultiHeadGATLayer(in_channels, hidden_channels, heads, dropout, concat=True)\n",
    "        )\n",
    "        \n",
    "        # Middle layers: (hidden * heads) -> hidden_channels\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                MultiHeadGATLayer(hidden_channels * heads, hidden_channels, heads, dropout, concat=True)\n",
    "            )\n",
    "        \n",
    "        # Last layer: average attention heads instead of concat\n",
    "        self.convs.append(\n",
    "            MultiHeadGATLayer(hidden_channels * heads, hidden_channels, heads, dropout, concat=False)\n",
    "        )\n",
    "        \n",
    "        # Readout dimension (mean + max)\n",
    "        self.readout_dim = hidden_channels * 2\n",
    "        \n",
    "        # MLP Classifier (like HCAN)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.readout_dim, hidden_channels),\n",
    "            nn.BatchNorm1d(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.BatchNorm1d(hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_channels // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        edge_attr = data.edge_attr if hasattr(data, 'edge_attr') else None\n",
    "        batch = data.batch if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        \n",
    "        # Multi-layer GAT feature extraction\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "        \n",
    "        # Graph-level readout: mean + max (captures global patterns)\n",
    "        x_mean = global_mean_pool(x, batch)\n",
    "        x_max = global_max_pool(x, batch)\n",
    "        graph_features = torch.cat([x_mean, x_max], dim=-1)\n",
    "        \n",
    "        # MLP classification\n",
    "        out = self.classifier(graph_features)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_attention_weights(self, data):\n",
    "        \"\"\"\n",
    "        Extract attention weights for interpretation\n",
    "        Shows which ROI connections are important\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        edge_attr = data.edge_attr if hasattr(data, 'edge_attr') else None\n",
    "        \n",
    "        attention_weights = []\n",
    "        for conv in self.convs:\n",
    "            # GATConv stores attention weights during forward pass\n",
    "            x = conv.gat(x, edge_index, edge_attr, return_attention_weights=True)\n",
    "            if isinstance(x, tuple):\n",
    "                x, (edge_idx, attn) = x\n",
    "                attention_weights.append((edge_idx, attn))\n",
    "                x = conv.bn(x)\n",
    "                x = F.elu(x)\n",
    "        \n",
    "        return attention_weights\n",
    "\n",
    "\n",
    "print(\"BrainGAT architecture defined\")\n",
    "print(\"  - Multi-head attention: 8 heads per layer\")\n",
    "print(\"  - 3-layer hierarchical feature extraction\")\n",
    "print(\"  - NO phenotypic data - pure imaging features!\")\n",
    "print(\"  - Attention mechanism learns important brain connections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76702bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ABIDE Dataset (Robust Loading)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "workspace_root = '/home/moew/Documents/ABIDE'\n",
    "data_dir = os.path.join(workspace_root, 'abide_data/Outputs/cpac/nofilt_noglobal/rois_cc400/')\n",
    "phenotype_file = os.path.join(workspace_root, 'Phenotypic_V1_0b_preprocessed1.csv')\n",
    "\n",
    "def load_abide_graphs(data_dir, phenotype_file, topk=20):\n",
    "    \"\"\"Load ABIDE subjects as graphs (NO PHENOTYPIC FEATURES!)\"\"\"\n",
    "    pheno_df = pd.read_csv(phenotype_file)\n",
    "    roi_files = sorted(glob.glob(f'{data_dir}/*.1D'))\n",
    "    \n",
    "    print(f\"Loading ABIDE data...\")\n",
    "    print(f\"   Phenotype CSV: {len(pheno_df)} subjects\")\n",
    "    print(f\"   .1D files found: {len(roi_files)}\")\n",
    "    print(f\"   Using ONLY imaging features (no demographics!)\")\n",
    "    \n",
    "    graphs, labels, subjects = [], [], []\n",
    "    \n",
    "    # Site mapping to handle naming discrepancies\n",
    "    site_map = {\n",
    "        'MaxMun': 'MAX_MUN',\n",
    "        'Leuven_1': 'LEUVEN_1',\n",
    "        'Leuven_2': 'LEUVEN_2',\n",
    "        'UCLA_1': 'UCLA_1',\n",
    "        'UCLA_2': 'UCLA_2',\n",
    "        'UM_1': 'UM_1',\n",
    "        'UM_2': 'UM_2',\n",
    "        'Trinity': 'TRINITY',\n",
    "        'Yale': 'YALE',\n",
    "        'Olin': 'OLIN',\n",
    "        'OHSU': 'OHSU',\n",
    "        'SBL': 'SBL',\n",
    "        'SDSU': 'SDSU',\n",
    "        'Stanford': 'STANFORD',\n",
    "        'Caltech': 'CALTECH',\n",
    "        'CMU': 'CMU',\n",
    "        'KKI': 'KKI',\n",
    "        'NYU': 'NYU',\n",
    "        'Pitt': 'PITT',\n",
    "        'USM': 'USM'\n",
    "    }\n",
    "    \n",
    "    for file_path in roi_files:\n",
    "        try:\n",
    "            filename = Path(file_path).stem\n",
    "            parts = filename.replace('_rois_cc400', '').split('_')\n",
    "            \n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Robust site parsing\n",
    "            site = parts[0]\n",
    "            subject_id_idx = 1\n",
    "            \n",
    "            # Check for multi-part site names (e.g., Leuven_1)\n",
    "            if len(parts) > 2 and parts[1].isdigit() and len(parts[1]) == 1:\n",
    "                 site = f\"{parts[0]}_{parts[1]}\"\n",
    "                 subject_id_idx = 2\n",
    "            \n",
    "            # Map site name\n",
    "            if site in site_map:\n",
    "                site = site_map[site]\n",
    "            elif site.upper() in site_map.values():\n",
    "                site = site.upper()\n",
    "                \n",
    "            # Find subject ID (first numeric part after site)\n",
    "            subject_id = None\n",
    "            for part in parts[subject_id_idx:]:\n",
    "                try:\n",
    "                    subject_id = int(part)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            if subject_id is None:\n",
    "                continue\n",
    "            \n",
    "            # Match to phenotype for LABEL ONLY\n",
    "            subject_row = pheno_df[\n",
    "                (pheno_df['SITE_ID'] == site) & \n",
    "                (pheno_df['SUB_ID'] == subject_id)\n",
    "            ]\n",
    "            \n",
    "            if not subject_row.empty:\n",
    "                dx_group = subject_row['DX_GROUP'].values[0]\n",
    "                \n",
    "                if dx_group in [1, 2]:\n",
    "                    ts = load_timeseries_1d(file_path)\n",
    "                    graph = graph_from_timeseries(ts, topk=topk)\n",
    "                    graph.y = torch.tensor([dx_group - 1], dtype=torch.long)\n",
    "                    \n",
    "                    graphs.append(graph)\n",
    "                    labels.append(dx_group - 1)\n",
    "                    subjects.append(f\"{site}_{subject_id}\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nLoaded {len(graphs)} subjects\")\n",
    "    print(f\"  - ASD: {labels.count(1)} | Control: {labels.count(0)}\")\n",
    "    if graphs:\n",
    "        print(f\"  - Graph: {graphs[0].x.shape[0]} nodes, {graphs[0].edge_index.shape[1]} edges\")\n",
    "        print(f\"  - Features: {graphs[0].x.shape[1]}-dim correlation vectors\")\n",
    "    \n",
    "    return graphs, labels, subjects\n",
    "\n",
    "# Load data with moderate connectivity (k=20)\n",
    "graphs, labels, subjects = load_abide_graphs(data_dir, phenotype_file, topk=20)\n",
    "\n",
    "# Stratified train/val/test split (70/15/15)\n",
    "train_val_graphs, test_graphs, train_val_labels, test_labels = train_test_split(\n",
    "    graphs, labels, test_size=0.15, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "train_graphs, val_graphs, train_labels, val_labels = train_test_split(\n",
    "    train_val_graphs, train_val_labels, test_size=0.176, random_state=42, stratify=train_val_labels\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = PyGDataLoader(train_graphs, batch_size=16, shuffle=True)\n",
    "val_loader = PyGDataLoader(val_graphs, batch_size=16, shuffle=False)\n",
    "test_loader = PyGDataLoader(test_graphs, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"\\nData split: {len(train_graphs)}/{len(val_graphs)}/{len(test_graphs)} (train/val/test)\")\n",
    "print(f\"Batch size: 16 (optimized for GPU)\")\n",
    "print(f\"Ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e934c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug Data Loading - Why are we losing subjects?\n",
    "\n",
    "def debug_data_loading(data_dir, phenotype_file):\n",
    "    print(\"\ud83d\udd75\ufe0f\u200d\u2640\ufe0f Debugging Data Loading Process...\")\n",
    "    \n",
    "    pheno_df = pd.read_csv(phenotype_file)\n",
    "    roi_files = sorted(glob.glob(f'{data_dir}/*.1D'))\n",
    "    \n",
    "    print(f\"   Total .1D files: {len(roi_files)}\")\n",
    "    print(f\"   Total Phenotype records: {len(pheno_df)}\")\n",
    "    \n",
    "    # Counters\n",
    "    stats = {\n",
    "        'loaded': 0,\n",
    "        'filename_parse_error': 0,\n",
    "        'no_subject_id': 0,\n",
    "        'pheno_mismatch': 0,\n",
    "        'invalid_dx': 0,\n",
    "        'load_error': 0\n",
    "    }\n",
    "    \n",
    "    # Check a few filenames to see format\n",
    "    print(f\"\\n   Sample filenames:\")\n",
    "    for f in roi_files[:5]:\n",
    "        print(f\"   - {Path(f).name}\")\n",
    "        \n",
    "    print(f\"\\n   Sample Phenotype Sites: {pheno_df['SITE_ID'].unique()[:5]}\")\n",
    "    \n",
    "    for file_path in roi_files:\n",
    "        try:\n",
    "            filename = Path(file_path).stem\n",
    "            # Attempt parsing\n",
    "            parts = filename.replace('_rois_cc400', '').split('_')\n",
    "            \n",
    "            if len(parts) < 2:\n",
    "                stats['filename_parse_error'] += 1\n",
    "                continue\n",
    "                \n",
    "            site = parts[0]\n",
    "            \n",
    "            # Find subject ID\n",
    "            subject_id = None\n",
    "            for part in parts[1:]:\n",
    "                try:\n",
    "                    subject_id = int(part)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            if subject_id is None:\n",
    "                stats['no_subject_id'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Check matching\n",
    "            # Try exact match first\n",
    "            subject_row = pheno_df[\n",
    "                (pheno_df['SITE_ID'] == site) & \n",
    "                (pheno_df['SUB_ID'] == subject_id)\n",
    "            ]\n",
    "            \n",
    "            if subject_row.empty:\n",
    "                # Try case insensitive site match\n",
    "                subject_row = pheno_df[\n",
    "                    (pheno_df['SITE_ID'].str.lower() == site.lower()) & \n",
    "                    (pheno_df['SUB_ID'] == subject_id)\n",
    "                ]\n",
    "            \n",
    "            if subject_row.empty:\n",
    "                stats['pheno_mismatch'] += 1\n",
    "                # Print first few mismatches to debug\n",
    "                if stats['pheno_mismatch'] <= 5:\n",
    "                    print(f\"   \u274c Mismatch: File={filename} -> Site={site}, ID={subject_id}\")\n",
    "                continue\n",
    "                \n",
    "            dx_group = subject_row['DX_GROUP'].values[0]\n",
    "            if dx_group not in [1, 2]:\n",
    "                stats['invalid_dx'] += 1\n",
    "                continue\n",
    "                \n",
    "            stats['loaded'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            stats['load_error'] += 1\n",
    "            \n",
    "    print(f\"\\n\ud83d\udcca Loading Statistics:\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"   {k:<20}: {v}\")\n",
    "        \n",
    "    return stats\n",
    "\n",
    "# Run debug\n",
    "debug_stats = debug_data_loading(data_dir, phenotype_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d304b395",
   "metadata": {},
   "source": [
    "## Ultra Low Memory Mode (RTX 2050 - 4GB VRAM)\n",
    "\n",
    "For RTX 2050 with 4GB VRAM, use this ultra-optimized configuration:\n",
    "\n",
    "### Memory Optimizations:\n",
    "- **Batch size 4**: Minimal memory footprint\n",
    "- **Smaller hidden**: 64 channels\n",
    "- **Fewer attention heads**: 4 heads\n",
    "- **Empty cache**: Clear GPU memory between batches\n",
    "\n",
    "### Memory Usage:\n",
    "- This config (batch=4, hidden=64): ~1.5-2GB VRAM (safe)\n",
    "\n",
    "### Performance Impact:\n",
    "- Training time: ~4-5 min/epoch\n",
    "- Accuracy: Should still reach ~60-70% (data-limited problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672da72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultra Low Memory Configuration for RTX 2050\n",
    "# Includes Early Stopping & Checkpointing\n",
    "\n",
    "# Get input dimensions from data\n",
    "sample_graph = train_graphs[0]\n",
    "in_features = sample_graph.x.size(1)  # 392 for CC400\n",
    "\n",
    "# Clear GPU memory first\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Cleared GPU cache\")\n",
    "\n",
    "print(\"\\nUltra Low Memory Mode\")\n",
    "print(\"   This configuration uses minimal VRAM (~1.5-2GB)\")\n",
    "\n",
    "# Tiny model: 64 hidden channels, 4 attention heads\n",
    "model_ultra = BrainGAT(\n",
    "    in_channels=in_features,\n",
    "    hidden_channels=64,   # Reduced from 128 \u2192 64\n",
    "    num_layers=3,\n",
    "    heads=4,              # Reduced from 8 \u2192 4\n",
    "    dropout=0.3,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "ultra_params = sum(p.numel() for p in model_ultra.parameters())\n",
    "print(f\"\\nUltra Low Memory Model:\")\n",
    "print(f\"   Parameters: {ultra_params:,}\")\n",
    "print(f\"   Memory: ~{ultra_params * 2 / 1e6:.0f} MB (FP16)\")\n",
    "\n",
    "# Smallest possible batch size\n",
    "print(f\"\\nUltra small batches:\")\n",
    "train_loader_ultra = PyGDataLoader(train_graphs, batch_size=4, shuffle=True)\n",
    "val_loader_ultra = PyGDataLoader(val_graphs, batch_size=4, shuffle=False)\n",
    "test_loader_ultra = PyGDataLoader(test_graphs, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"   Batch size: 4 (absolute minimum)\")\n",
    "print(f\"   Train batches: {len(train_loader_ultra)}\")\n",
    "\n",
    "# --- Checkpointing & Early Stopping Classes ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    def __init__(self, filepath='models/best_model.pth'):\n",
    "        self.filepath = filepath\n",
    "        self.best_acc = 0.0\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "    def __call__(self, model, val_acc, epoch):\n",
    "        if val_acc > self.best_acc:\n",
    "            self.best_acc = val_acc\n",
    "            torch.save(model.state_dict(), self.filepath)\n",
    "            print(f\"   \u2713 New best model saved! (Acc: {val_acc:.2f}%)\")\n",
    "        \n",
    "        # Also save latest checkpoint for crash recovery\n",
    "        latest_path = self.filepath.replace('best_model', 'latest_checkpoint')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_acc': self.best_acc\n",
    "        }, latest_path)\n",
    "\n",
    "# Setup Training\n",
    "optimizer = torch.optim.Adam(model_ultra.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(patience=15, min_delta=0.001)\n",
    "checkpoint = ModelCheckpoint(filepath='models/braingat_ultra_best.pth')\n",
    "\n",
    "print(f\"\\n\ud83d\ude80 Training with ultra low memory settings...\")\n",
    "print(f\"   Batch size: 4 (smallest possible)\")\n",
    "print(f\"   Expected: ~4-5 min/epoch\")\n",
    "print(f\"   This WILL work on 4GB VRAM!\")\n",
    "\n",
    "# Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model_ultra.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data in train_loader_ultra:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model_ultra(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "        total += data.num_graphs\n",
    "        \n",
    "    train_loss = total_loss / total\n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model_ultra.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader_ultra:\n",
    "            data = data.to(device)\n",
    "            out = model_ultra(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            val_loss += loss.item() * data.num_graphs\n",
    "            pred = out.argmax(dim=1)\n",
    "            val_correct += int((pred == data.y).sum())\n",
    "            val_total += data.num_graphs\n",
    "    \n",
    "    val_loss /= val_total\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    \n",
    "    print(f\"Epoch {epoch:03d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint(model_ultra, val_acc, epoch)\n",
    "    \n",
    "    # Early Stopping\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"\u23f9\ufe0f Early stopping triggered!\")\n",
    "        break\n",
    "        \n",
    "print(\"\u2705 Ultra low memory training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ultra Low Memory Model\n",
    "\n",
    "print(\"Training with ultra low memory settings...\")\n",
    "print(\"   Batch size: 4 (smallest possible)\")\n",
    "print(\"   Expected: ~4-5 min/epoch\")\n",
    "print()\n",
    "\n",
    "# Custom training function with aggressive memory management\n",
    "def train_ultra_low_memory(model, train_loader, val_loader, num_epochs=100, lr=0.0005, patience=20):\n",
    "    \"\"\"Training with aggressive memory management for low VRAM\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=7, verbose=True\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    use_amp = device.type == 'cuda'\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'best_val_acc': 0.0, 'best_epoch': 0\n",
    "    }\n",
    "    \n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Ultra Low Memory Training\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Memory optimization: MAXIMUM\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Epoch':<8}{'Train Loss':<12}{'Train Acc':<12}{'Val Loss':<12}{'Val Acc':<12}{'Status'}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # === TRAINING ===\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    out = model(batch)\n",
    "                    loss = criterion(out, batch.y.squeeze())\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                out = model(batch)\n",
    "                loss = criterion(out, batch.y.squeeze())\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch.num_graphs\n",
    "            pred = out.argmax(dim=1)\n",
    "            train_correct += (pred == batch.y.squeeze()).sum().item()\n",
    "            train_total += batch.num_graphs\n",
    "            \n",
    "            # Clear cache every 10 batches to prevent fragmentation\n",
    "            if use_amp and batch_idx % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total * 100\n",
    "        \n",
    "        # === VALIDATION ===\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        out = model(batch)\n",
    "                        loss = criterion(out, batch.y.squeeze())\n",
    "                else:\n",
    "                    out = model(batch)\n",
    "                    loss = criterion(out, batch.y.squeeze())\n",
    "                \n",
    "                val_loss += loss.item() * batch.num_graphs\n",
    "                pred = out.argmax(dim=1)\n",
    "                val_correct += (pred == batch.y.squeeze()).sum().item()\n",
    "                val_total += batch.num_graphs\n",
    "        \n",
    "        # Clear cache after validation\n",
    "        if use_amp:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total * 100\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        status = \"\"\n",
    "        if val_acc > history['best_val_acc']:\n",
    "            history['best_val_acc'] = val_acc\n",
    "            history['best_epoch'] = epoch\n",
    "            torch.save(model.state_dict(), 'braingat_ultra.pth')\n",
    "            patience_counter = 0\n",
    "            status = \"Best\"\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            status = f\"({patience_counter}/{patience})\"\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0 or patience_counter == 0 or patience_counter >= patience:\n",
    "            print(f\"{epoch+1:<8}{train_loss:<12.4f}{train_acc:<12.2f}{val_loss:<12.4f}{val_acc:<12.2f}{status}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load('braingat_ultra.pth'))\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Training complete!\")\n",
    "    print(f\"  Best val accuracy: {history['best_val_acc']:.2f}% (epoch {history['best_epoch']+1})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train the ultra low memory model\n",
    "trained_model_ultra, train_history_ultra = train_ultra_low_memory(\n",
    "    model_ultra,\n",
    "    train_loader_ultra,\n",
    "    val_loader_ultra,\n",
    "    num_epochs=100,\n",
    "    lr=0.0005,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "print(\"\\nUltra low memory training complete!\")\n",
    "print(f\"Model saved to: braingat_ultra.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829afb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ultra Low Memory Model\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_gat(model, test_loader, device):\n",
    "    \"\"\"Comprehensive test set evaluation\"\"\"\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            \n",
    "            probs = F.softmax(out, dim=1)\n",
    "            preds = out.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch.y.squeeze().cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    accuracy = (np.array(all_preds) == np.array(all_labels)).mean() * 100\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SET EVALUATION (Held-Out Data)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"ROC AUC Score: {auc:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Control', 'ASD'], digits=4))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Control', 'ASD'],\n",
    "                yticklabels=['Control', 'ASD'])\n",
    "    plt.title(f'BrainGAT Test Confusion Matrix\\nAccuracy: {accuracy:.2f}%, AUC: {auc:.4f}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probs\n",
    "    }\n",
    "\n",
    "print(\"Evaluating ultra low memory model...\")\n",
    "test_results_ultra = evaluate_gat(trained_model_ultra, test_loader_ultra, device)\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ULTRA LOW MEMORY PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"   Hidden channels: 64 (smallest)\")\n",
    "print(f\"   Attention heads: 4 (reduced)\")\n",
    "print(f\"   Parameters: {ultra_params:,}\")\n",
    "print(f\"   Batch size: 4 (minimum)\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"   Test Accuracy: {test_results_ultra['accuracy']:.2f}%\")\n",
    "print(f\"   ROC-AUC: {test_results_ultra['auc']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096ab27",
   "metadata": {},
   "source": [
    "## Improved Training: More Epochs + Elastic Net Regularization\n",
    "\n",
    "Let's train properly with:\n",
    "\n",
    "### Key Improvements:\n",
    "1. **More Epochs**: 200 epochs (vs 100) - model needs more time\n",
    "2. **Elastic Net**: L1 + L2 regularization (prevents overfitting)\n",
    "3. **Lower Learning Rate**: 0.0001 (vs 0.0005) - more stable convergence\n",
    "4. **Longer Patience**: 30 epochs (vs 20) - don't give up too early\n",
    "5. **Stronger Dropout**: 0.4 (vs 0.3) - better generalization\n",
    "\n",
    "### Why This Helps:\n",
    "- Previous run: 85% train, 59% val \u2192 massive overfitting\n",
    "- Elastic net (L1+L2) penalizes large weights and promotes sparsity\n",
    "- Lower LR prevents getting stuck in bad local minima\n",
    "- More epochs gives model time to find better solutions\n",
    "\n",
    "### Expected Time:\n",
    "- ~5 min/epoch \u00d7 60-80 epochs = 5-7 hours\n",
    "- Will early stop when converged (won't run all 200 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Improved Model with Stronger Regularization\n",
    "\n",
    "print(\"Creating improved model with elastic net regularization...\")\n",
    "\n",
    "# Clear memory first\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Cleared GPU cache\")\n",
    "\n",
    "# Model with STRONGER dropout for better regularization\n",
    "model_improved = BrainGAT(\n",
    "    in_channels=in_features,\n",
    "    hidden_channels=64,   # Same as ultra\n",
    "    num_layers=3,\n",
    "    heads=4,              # Same as ultra\n",
    "    dropout=0.4,          # INCREASED from 0.3 \u2192 0.4\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "improved_params = sum(p.numel() for p in model_improved.parameters())\n",
    "print(f\"\\nImproved Model Configuration:\")\n",
    "print(f\"   Parameters: {improved_params:,}\")\n",
    "print(f\"   Dropout: 0.4 (stronger regularization)\")\n",
    "print(f\"   Ready for elastic net training!\")\n",
    "print(f\"\\nModel initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f72b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with Elastic Net (L1 + L2) Regularization\n",
    "\n",
    "def train_with_elastic_net(model, train_loader, val_loader, \n",
    "                           num_epochs=200, lr=0.0001, \n",
    "                           l1_lambda=1e-5, l2_lambda=5e-4,\n",
    "                           patience=30):\n",
    "    \"\"\"\n",
    "    Training with Elastic Net regularization (L1 + L2)\n",
    "    \n",
    "    Elastic Net = L1 (Lasso) + L2 (Ridge)\n",
    "    - L1: Promotes sparsity (sets some weights to zero)\n",
    "    - L2: Prevents large weights (smoothness)\n",
    "    - Together: Better generalization than either alone!\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # AdamW for L2, we'll add L1 manually\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=l2_lambda)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=10, verbose=True, min_lr=1e-6\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    use_amp = device.type == 'cuda'\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'learning_rates': [],\n",
    "        'best_val_acc': 0.0, 'best_epoch': 0\n",
    "    }\n",
    "    \n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training with Elastic Net Regularization\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"L1 lambda: {l1_lambda} (sparsity)\")\n",
    "    print(f\"L2 lambda: {l2_lambda} (smoothness)\")\n",
    "    print(f\"Dropout: 0.4 (strong regularization)\")\n",
    "    print(f\"Patience: {patience} epochs\")\n",
    "    print(f\"Max epochs: {num_epochs}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Epoch':<8}{'Train Loss':<12}{'Train Acc':<12}{'Val Loss':<12}{'Val Acc':<12}{'LR':<12}{'Status'}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # === TRAINING ===\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    out = model(batch)\n",
    "                    loss = criterion(out, batch.y.squeeze())\n",
    "                    \n",
    "                    # Add L1 regularization (manually)\n",
    "                    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                    loss = loss + l1_lambda * l1_norm\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                out = model(batch)\n",
    "                loss = criterion(out, batch.y.squeeze())\n",
    "                \n",
    "                # Add L1 regularization\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss = loss + l1_lambda * l1_norm\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch.num_graphs\n",
    "            pred = out.argmax(dim=1)\n",
    "            train_correct += (pred == batch.y.squeeze()).sum().item()\n",
    "            train_total += batch.num_graphs\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if use_amp and batch_idx % 15 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total * 100\n",
    "        \n",
    "        # === VALIDATION ===\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        out = model(batch)\n",
    "                        loss = criterion(out, batch.y.squeeze())\n",
    "                else:\n",
    "                    out = model(batch)\n",
    "                    loss = criterion(out, batch.y.squeeze())\n",
    "                \n",
    "                val_loss += loss.item() * batch.num_graphs\n",
    "                pred = out.argmax(dim=1)\n",
    "                val_correct += (pred == batch.y.squeeze()).sum().item()\n",
    "                val_total += batch.num_graphs\n",
    "        \n",
    "        if use_amp:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total * 100\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        status = \"\"\n",
    "        if val_acc > history['best_val_acc']:\n",
    "            history['best_val_acc'] = val_acc\n",
    "            history['best_epoch'] = epoch\n",
    "            torch.save(model.state_dict(), 'braingat_elastic_net.pth')\n",
    "            patience_counter = 0\n",
    "            status = \"Best\"\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            status = f\"({patience_counter}/{patience})\"\n",
    "        \n",
    "        # Print every 5 epochs or important events\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0 or patience_counter == 0 or patience_counter >= patience:\n",
    "            print(f\"{epoch+1:<8}{train_loss:<12.4f}{train_acc:<12.2f}{val_loss:<12.4f}{val_acc:<12.2f}{current_lr:<12.2e}{status}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            print(f\"   No improvement for {patience} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('braingat_elastic_net.pth'))\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Training complete!\")\n",
    "    print(f\"  Best validation accuracy: {history['best_val_acc']:.2f}% (epoch {history['best_epoch']+1})\")\n",
    "    print(f\"  Total epochs trained: {len(history['train_loss'])}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Elastic net training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd8dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Improved Model with Elastic Net\n",
    "\n",
    "print(\"Starting long training run with elastic net regularization...\")\n",
    "print(\"   This will take 5-7 hours but should achieve better results!\")\n",
    "print()\n",
    "print(\"What's happening:\")\n",
    "print(\"   - L1 regularization: Promoting sparse weights (feature selection)\")\n",
    "print(\"   - L2 regularization: Preventing large weights (smoothness)\")\n",
    "print(\"   - Dropout 0.4: Strong generalization\")\n",
    "print(\"   - Lower LR (0.0001): More stable convergence\")\n",
    "print(\"   - Patience 30: Won't give up too easily\")\n",
    "print()\n",
    "\n",
    "# Train with elastic net\n",
    "trained_model_improved, history_improved = train_with_elastic_net(\n",
    "    model_improved,\n",
    "    train_loader_ultra,\n",
    "    val_loader_ultra,\n",
    "    num_epochs=200,       # More epochs\n",
    "    lr=0.0001,            # Lower learning rate\n",
    "    l1_lambda=1e-5,       # L1 regularization strength\n",
    "    l2_lambda=5e-4,       # L2 regularization strength\n",
    "    patience=30           # Longer patience\n",
    ")\n",
    "\n",
    "print(\"\\nLong training complete!\")\n",
    "print(f\"Best model saved to: braingat_elastic_net.pth\")\n",
    "print(f\"Best validation accuracy: {history_improved['best_val_acc']:.2f}%\")\n",
    "print(f\"Total epochs: {len(history_improved['train_loss'])}\")\n",
    "print(f\"\\nReady for test evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a97fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Improved Model with Elastic Net\n",
    "\n",
    "print(\"Evaluating improved model with elastic net...\")\n",
    "test_results_improved = evaluate_gat(trained_model_improved, test_loader_ultra, device)\n",
    "\n",
    "# Comprehensive comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Model':<30}{'Val Acc':<15}{'Test Acc':<15}{'Test AUC':<15}{'Epochs'}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Ultra (quick, 22 epochs)':<30}{train_history_ultra['best_val_acc']:<15.2f}{test_results_ultra['accuracy']:<15.2f}{test_results_ultra['auc']:<15.4f}{len(train_history_ultra['train_loss'])}\")\n",
    "print(f\"{'Improved (elastic net)':<30}{history_improved['best_val_acc']:<15.2f}{test_results_improved['accuracy']:<15.2f}{test_results_improved['auc']:<15.4f}{len(history_improved['train_loss'])}\")\n",
    "\n",
    "improvement = test_results_improved['accuracy'] - test_results_ultra['accuracy']\n",
    "print(f\"\\n{'Improvement:':<30}{'':<15}{improvement:+.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if test_results_improved['accuracy'] >= 65:\n",
    "    print(\"EXCELLENT! Achieved 65%+ accuracy (expected range for pure imaging)\")\n",
    "    print(\"   This is competitive with published ASD classification papers!\")\n",
    "elif test_results_improved['accuracy'] >= 60:\n",
    "    print(\"GOOD! Achieved 60-65% accuracy (solid performance)\")\n",
    "    print(\"   Better than random chance, meaningful signal detected!\")\n",
    "elif test_results_improved['accuracy'] >= 55:\n",
    "    print(\"FAIR performance (55-60%). Model learned something but not optimal.\")\n",
    "else:\n",
    "    print(\"Below expectations. Model struggled to learn patterns.\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"   - Train-val gap at best epoch: {history_improved['train_acc'][history_improved['best_epoch']] - history_improved['best_val_acc']:.2f}%\")\n",
    "if history_improved['train_acc'][history_improved['best_epoch']] - history_improved['best_val_acc'] < 10:\n",
    "    print(f\"     Good generalization (gap < 10%)\")\n",
    "else:\n",
    "    print(f\"     Still some overfitting (gap > 10%)\")\n",
    "\n",
    "print(f\"   - Learning rate at end: {history_improved['learning_rates'][-1]:.2e}\")\n",
    "print(f\"   - Patience used: {len(history_improved['train_loss']) - history_improved['best_epoch'] - 1}/30\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169bf2c",
   "metadata": {},
   "source": [
    "## Troubleshooting OOM Errors\n",
    "\n",
    "If you still get Out of Memory errors:\n",
    "\n",
    "### 1. Restart Kernel First\n",
    "Restart the kernel to clear all GPU memory, then run ONLY the ultra low memory cells.\n",
    "\n",
    "### 2. Close Other Applications\n",
    "- Close Chrome/Firefox (huge VRAM usage)\n",
    "- Close other Python notebooks\n",
    "- Stop any GPU-using processes: `nvidia-smi` to check\n",
    "\n",
    "### 3. Set Environment Variable\n",
    "```python\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "# Run this BEFORE creating any models\n",
    "```\n",
    "\n",
    "### 4. Even Smaller Batch Size (batch=2)\n",
    "```python\n",
    "# Absolute minimum - only if batch=4 still fails\n",
    "train_loader_minimal = PyGDataLoader(train_graphs, batch_size=2, shuffle=True)\n",
    "val_loader_minimal = PyGDataLoader(val_graphs, batch_size=2, shuffle=False)\n",
    "```\n",
    "\n",
    "### 5. CPU Fallback (Last Resort)\n",
    "```python\n",
    "# If GPU absolutely won't work, train on CPU\n",
    "device = torch.device('cpu')\n",
    "# Expect: ~10-15 min/epoch instead of 4-5 min\n",
    "# But it WILL work and achieve same accuracy!\n",
    "```\n",
    "\n",
    "### 6. Monitor Memory Usage\n",
    "```python\n",
    "# Check GPU memory during training\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "    print(f\"Max allocated: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70afbfcc",
   "metadata": {},
   "source": [
    "## Summary: BrainGAT vs HCAN Comparison\n",
    "\n",
    "### HCAN (Original Paper)\n",
    "- Heterogeneous graph: ROI nodes + phenotype nodes\n",
    "- Uses demographic features (age, sex, site, IQ)\n",
    "- Population-level graph across subjects\n",
    "- Risk of data leakage from phenotypic features\n",
    "\n",
    "### Our BrainGAT (Pure Imaging)\n",
    "- Homogeneous graph: ROI nodes only\n",
    "- NO phenotypic data - only fMRI connectivity\n",
    "- Subject-level graphs (proper data splitting)\n",
    "- Multi-head attention for learning important connections\n",
    "- GPU-optimized with mixed precision training\n",
    "\n",
    "### Key Advantages\n",
    "1. **No Data Leakage**: Pure imaging features only\n",
    "2. **Interpretable**: Attention weights show important connections\n",
    "3. **GPU-Optimized**: Faster training on RTX 2050/CUDA\n",
    "4. **Scientifically Rigorous**: Proper train/val/test splits\n",
    "\n",
    "### Expected Performance\n",
    "- **HCAN (with phenotype)**: ~75-85% accuracy\n",
    "- **BrainGAT (imaging only)**: ~65-75% accuracy (realistic!)\n",
    "- Lower accuracy is EXPECTED without phenotypic shortcuts\n",
    "- Still competitive with proper methodology!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}